# =============================================================================
# Configuration de l'application FastAPI GraphQL LLM
# Copiez ce fichier vers .env et adaptez les valeurs à votre environnement
# =============================================================================

# -----------------------------------------------------------------------------
# Configuration du Projet
# -----------------------------------------------------------------------------
PROJECT_NAME=FastAPI GraphQL LLM Project
DEBUG=false

# -----------------------------------------------------------------------------
# Configuration du Serveur
# -----------------------------------------------------------------------------
HOST=0.0.0.0
PORT=8001
LOG_LEVEL=info
# Nombre de workers gunicorn (ajuster selon les ressources CPU disponibles)
WORKERS=1

# -----------------------------------------------------------------------------
# Configuration LLM (LM Studio ou compatible OpenAI)
# -----------------------------------------------------------------------------
# URL du serveur LLM (LM Studio, Ollama, OpenAI, etc.)
LLM_CHAT_SERVER_BASE_URL=http://localhost:1234/v1
# Nom du modèle à utiliser
LLM_CHAT_MODEL=qwen3-4b-instruct-2507
# Température (0.0 = déterministe, 1.0 = créatif)
LLM_CHAT_TEMPERATURE=0.3
# Clé API (si nécessaire, sinon laisser "not-needed")
LLM_CHAT_API_KEY=not-needed
# Timeout en secondes pour les requêtes LLM
LLM_CHAT_TIMEOUT=60

# -----------------------------------------------------------------------------
# Configuration de l'API Movie externe
# -----------------------------------------------------------------------------
# URL de l'API Movie (microservice TP1)
MOVIE_API_BASE_URL=http://localhost:8000/api/v1
# Timeout en secondes pour les requêtes à l'API Movie
MOVIE_API_TIMEOUT=30

# -----------------------------------------------------------------------------
# Configuration Health Check
# -----------------------------------------------------------------------------
HEALTH_CHECK_PATH=/health

# =============================================================================
# Exemples de configuration pour différents environnements
# =============================================================================

# --- Développement local ---
# LLM_CHAT_SERVER_BASE_URL=http://localhost:1234/v1
# MOVIE_API_BASE_URL=http://localhost:8000/api/v1

# --- Docker Compose ---
# LLM_CHAT_SERVER_BASE_URL=http://lm-studio:1234/v1
# MOVIE_API_BASE_URL=http://movie-api:8000/api/v1

# --- Kubernetes ---
# LLM_CHAT_SERVER_BASE_URL=http://lm-studio-service.default.svc.cluster.local:1234/v1
# MOVIE_API_BASE_URL=http://movie-api-service.default.svc.cluster.local:8000/api/v1
